{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLjWKo45xTkAM83mHccKYn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar045/Assignment/blob/main/Readability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from openai import OpenAI\n",
        "import textstat\n",
        "import backoff\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Load and combine datasets\n",
        "def load_and_combine_datasets():\n",
        "    # Load CLEAR dataset\n",
        "    clear_df = pd.read_csv(\"path_to_your_CLEAR_dataset.csv\")\n",
        "    clear_df['dataset'] = 'CLEAR'\n",
        "    clear_df['language'] = 'en'\n",
        "\n",
        "    # Load ratings dataset\n",
        "    ratings_df = pd.read_csv(\"ratings.csv\", encoding=\"iso-8859-1\")\n",
        "    ratings_df.rename(lambda x: str(x).lower(), axis=\"columns\", inplace=True)\n",
        "    ratings_df['dataset'] = 'ratings'\n",
        "    ratings_df['language'] = 'de'\n",
        "    ratings_df.rename(columns={'sentence': 'Excerpt'}, inplace=True)\n",
        "\n",
        "    # Combine datasets\n",
        "    combined_df = pd.concat([clear_df, ratings_df], ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def estimate_bt_easiness(original_text):\n",
        "    prompt = f\"\"\"Estimate the BT_easiness score for the following text. BT_easiness is a measure of text readability,\n",
        "    where higher scores indicate easier-to-read text. The score typically ranges from 0 to 100.\n",
        "\n",
        "    Text:\n",
        "    {original_text}\n",
        "\n",
        "    Based on the complexity, vocabulary, and structure of the text, estimate the BT_easiness score.\n",
        "    Provide only the numeric score without any explanation.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return float(response.choices[0].message.content.strip())\n",
        "\n",
        "# Bilingual readability metrics\n",
        "def calculate_readability_scores(text, language, row=None):\n",
        "    if language == 'en':\n",
        "        bt_easiness = estimate_bt_easiness(text)\n",
        "        return {\n",
        "            \"BT_easiness\": bt_easiness,\n",
        "            \"Flesch-Reading-Ease\": textstat.flesch_reading_ease(text),\n",
        "            \"SMOG Readability\": textstat.smog_index(text),\n",
        "            \"Automated Readability Index\": textstat.automated_readability_index(text)\n",
        "        }\n",
        "    elif language == 'de':\n",
        "        return {\n",
        "            \"mos_complexity\": row['mos_complexity'] if row is not None else 0,\n",
        "            \"votes_complexity\": row['votes_complexity'] if row is not None else 0,\n",
        "            \"votes_understandability\": row['votes_understandability'] if row is not None else 0,\n",
        "            \"vote_lexical_difficulty\": row['vote_lexical_difficulty'] if row is not None else 0\n",
        "        }\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def modify_text_gpt4(text, current_scores, language):\n",
        "    if language == 'en':\n",
        "        prompt = f\"\"\"Improve the readability of the following text. Current metrics:\n",
        "        - BT_easiness: {current_scores['BT_easiness']:.2f}\n",
        "        - Flesch-Reading-Ease: {current_scores['Flesch-Reading-Ease']:.2f}\n",
        "        - SMOG Readability: {current_scores['SMOG Readability']:.2f}\n",
        "        - Automated Readability Index: {current_scores['Automated Readability Index']:.2f}\n",
        "\n",
        "    Original text:\n",
        "    {text}\n",
        "\n",
        "    Rewrite the text to improve all readability metrics. Aim for:\n",
        "        - Higher BT_easiness\n",
        "        - Higher Flesch-Reading-Ease\n",
        "        - Lower SMOG Readability\n",
        "        - Lower Automated Readability Index\n",
        "\n",
        "    Improved text:\"\"\"\n",
        "    elif language == 'de':\n",
        "        prompt = f\"\"\"Verbessere die Lesbarkeit des folgenden Textes. Aktuelle Metriken:\n",
        "        - MOS Komplexität: {current_scores['mos_complexity']:.2f}\n",
        "        - Komplexitätsbewertungen: {current_scores['votes_complexity']}\n",
        "        - Verständlichkeitsbewertungen: {current_scores['votes_understandability']}\n",
        "        - Lexikalische Schwierigkeit: {current_scores['vote_lexical_difficulty']}\n",
        "\n",
        "    Originaltext:\n",
        "    {text}\n",
        "\n",
        "    Schreibe den Text um, um alle Lesbarkeitsmetriken zu verbessern. Ziele:\n",
        "        - Niedrigere MOS Komplexität\n",
        "        - Weniger Komplexitätsbewertungen\n",
        "        - Mehr Verständlichkeitsbewertungen\n",
        "        - Niedrigere lexikalische Schwierigkeit\n",
        "\n",
        "    Verbesserter Text:\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def estimate_german_metrics(original_text, modified_text, original_scores):\n",
        "    prompt = f\"\"\"Given the original German text and its improved version, estimate the new readability metrics.\n",
        "    The original metrics were:\n",
        "    - MOS Komplexität: {original_scores['mos_complexity']:.2f}\n",
        "    - Komplexitätsbewertungen: {original_scores['votes_complexity']}\n",
        "    - Verständlichkeitsbewertungen: {original_scores['votes_understandability']}\n",
        "    - Lexikalische Schwierigkeit: {original_scores['vote_lexical_difficulty']}\n",
        "\n",
        "    Original text:\n",
        "    {original_text}\n",
        "\n",
        "    Improved text:\n",
        "    {modified_text}\n",
        "\n",
        "    Estimate the new metrics. They should show improvement over the original scores.\n",
        "    Provide only the numeric scores in the following format:\n",
        "    MOS Komplexität: [value]\n",
        "    Komplexitätsbewertungen: [value]\n",
        "    Verständlichkeitsbewertungen: [value]\n",
        "    Lexikalische Schwierigkeit: [value]\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    # Parse the response\n",
        "    lines = response.choices[0].message.content.strip().split('\\n')\n",
        "    new_scores = {}\n",
        "    for line in lines:\n",
        "        key, value = line.split(':')\n",
        "        new_scores[key.strip()] = float(value.strip())\n",
        "\n",
        "    return {\n",
        "        \"mos_complexity\": new_scores[\"MOS Komplexität\"],\n",
        "        \"votes_complexity\": int(new_scores[\"Komplexitätsbewertungen\"]),\n",
        "        \"votes_understandability\": int(new_scores[\"Verständlichkeitsbewertungen\"]),\n",
        "        \"vote_lexical_difficulty\": int(new_scores[\"Lexikalische Schwierigkeit\"])\n",
        "    }\n",
        "\n",
        "# Data preparation function\n",
        "def prepare_data(df, sample_size=1000):\n",
        "    df_subset = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "    prepared_data = []\n",
        "\n",
        "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=\"Preparing data\"):\n",
        "        original_scores = calculate_readability_scores(row['Excerpt'], row['language'], row)\n",
        "\n",
        "        gpt4_modified_text = modify_text_gpt4(row['Excerpt'], original_scores, row['language'])\n",
        "        if row['language'] == 'en':\n",
        "            gpt4_scores = calculate_readability_scores(gpt4_modified_text, row['language'])\n",
        "        else:\n",
        "            gpt4_scores = estimate_german_metrics(row['Excerpt'], gpt4_modified_text, original_scores)\n",
        "\n",
        "        prepared_data.append({\n",
        "            \"original_text\": row['Excerpt'],\n",
        "            \"original_scores\": original_scores,\n",
        "            \"gpt4_text\": gpt4_modified_text,\n",
        "            \"gpt4_scores\": gpt4_scores,\n",
        "            \"language\": row['language']\n",
        "        })\n",
        "\n",
        "    return prepared_data\n",
        "\n",
        "# Llama 2 model setup and training functions\n",
        "def setup_llama2_model(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\"]\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    return model, tokenizer\n",
        "\n",
        "def train_llama2(model, tokenizer, train_data, output_dir, num_epochs=3):\n",
        "    train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
        "\n",
        "    tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_steps=100,\n",
        "        save_steps=1000,\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model\n",
        "\n",
        "# Evaluation and feedback functions\n",
        "def evaluate_improvement(original_text, gpt4_text, llama2_text, original_scores, gpt4_scores, language):\n",
        "    if language == 'en':\n",
        "        llama2_scores = calculate_readability_scores(llama2_text, language)\n",
        "    else:\n",
        "        llama2_scores = estimate_german_metrics(original_text, llama2_text, original_scores)\n",
        "\n",
        "    improvements = {}\n",
        "    metrics = list(original_scores.keys())\n",
        "    for metric in metrics:\n",
        "        original_val = original_scores[metric]\n",
        "        gpt4_val = gpt4_scores[metric]\n",
        "        llama2_val = llama2_scores[metric]\n",
        "\n",
        "        if language == 'en' and metric in ['BT_easiness', 'Flesch-Reading-Ease']:\n",
        "            gpt4_improvement = (gpt4_val - original_val) / original_val * 100\n",
        "            llama2_improvement = (llama2_val - original_val) / original_val * 100\n",
        "        elif language == 'de' and metric in ['votes_understandability']:\n",
        "            gpt4_improvement = (gpt4_val - original_val) / original_val * 100\n",
        "            llama2_improvement = (llama2_val - original_val) / original_val * 100\n",
        "        else:\n",
        "            gpt4_improvement = (original_val - gpt4_val) / original_val * 100\n",
        "            llama2_improvement = (original_val - llama2_val) / original_val * 100\n",
        "\n",
        "        improvements[metric] = {\n",
        "            'gpt4': gpt4_improvement,\n",
        "            'llama2': llama2_improvement\n",
        "        }\n",
        "\n",
        "    return llama2_scores, improvements\n",
        "\n",
        "def generate_feedback(improvements, language):\n",
        "    feedback = \"Based on the improvements:\\n\" if language == 'en' else \"Basierend auf den Verbesserungen:\\n\"\n",
        "    for metric, values in improvements.items():\n",
        "        if values['llama2'] >= values['gpt4']:\n",
        "            feedback += f\"- {metric}: {'Great job! You\\'ve matched or exceeded GPT-4\\'s improvement.' if language == 'en' else 'Großartig! Sie haben die Verbesserung von GPT-4 erreicht oder übertroffen.'}\\n\"\n",
        "        else:\n",
        "            feedback += f\"- {metric}: {'There\\'s room for improvement. Try to match GPT-4\\'s performance.' if language == 'en' else 'Es gibt Raum für Verbesserungen. Versuchen Sie, die Leistung von GPT-4 zu erreichen.'}\\n\"\n",
        "    return feedback\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def get_gpt4_analysis(original_text, gpt4_text, llama2_text, improvements, language):\n",
        "    prompt = f\"\"\"Analyze the readability improvements made by Llama 2 compared to GPT-4 for the following {'English' if language == 'en' else 'German'} text.\n",
        "\n",
        "Original text:\n",
        "{original_text}\n",
        "\n",
        "GPT-4 improved version:\n",
        "{gpt4_text}\n",
        "\n",
        "Llama 2 improved version:\n",
        "{llama2_text}\n",
        "\n",
        "Improvement percentages:\n",
        "{improvements}\n",
        "\n",
        "Provide a detailed analysis of Llama 2's performance:\n",
        "1. What did Llama 2 do well in improving readability?\n",
        "2. Where did Llama 2 fall short compared to GPT-4?\n",
        "3. Suggest specific strategies for Llama 2 to improve its performance.\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Continuing from the feedback_loop_training function\n",
        "def feedback_loop_training(model, tokenizer, dataset, num_iterations=5):\n",
        "    for iteration in range(num_iterations):\n",
        "        logging.info(f\"Starting iteration {iteration + 1}\")\n",
        "\n",
        "        new_training_data = []\n",
        "        for item in tqdm(dataset, desc=f\"Processing iteration {iteration + 1}\"):\n",
        "            llama2_text = improve_readability(item['original_text'], item['original_scores'],\n",
        "                                              item['gpt4_text'], item['gpt4_scores'], model, tokenizer, item['language'])\n",
        "\n",
        "            llama2_scores, improvements = evaluate_improvement(item['original_text'], item['gpt4_text'],\n",
        "                                                               llama2_text, item['original_scores'], item['gpt4_scores'], item['language'])\n",
        "\n",
        "            feedback = generate_feedback(improvements, item['language'])\n",
        "            gpt4_analysis = get_gpt4_analysis(item['original_text'], item['gpt4_text'], llama2_text, improvements, item['language'])\n",
        "\n",
        "            new_example = f\"\"\"{'Original text' if item['language'] == 'en' else 'Originaltext'}: {item['original_text']}\n",
        "\n",
        "{'GPT-4 improved version' if item['language'] == 'en' else 'Von GPT-4 verbesserte Version'}: {item['gpt4_text']}\n",
        "\n",
        "{'Your previous improvement' if item['language'] == 'en' else 'Ihre vorherige Verbesserung'}: {llama2_text}\n",
        "\n",
        "{'Feedback' if item['language'] == 'en' else 'Rückmeldung'}: {feedback}\n",
        "\n",
        "{'Expert analysis' if item['language'] == 'en' else 'Expertenanalyse'}: {gpt4_analysis}\n",
        "\n",
        "{'Now, provide an improved version addressing the feedback and analysis' if item['language'] == 'en' else 'Geben Sie nun eine verbesserte Version an, die auf das Feedback und die Analyse eingeht'}:\n",
        "\n",
        "{'Improved text' if item['language'] == 'en' else 'Verbesserter Text'}:\"\"\"\n",
        "\n",
        "            new_training_data.append({\"text\": new_example})\n",
        "\n",
        "        # Train on new data\n",
        "        output_dir = f\"./llama2_readability_improvement_iteration_{iteration + 1}\"\n",
        "        model = train_llama2(model, tokenizer, new_training_data, output_dir, num_epochs=1)\n",
        "\n",
        "        logging.info(f\"Completed iteration {iteration + 1}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Utility function for using the trained model\n",
        "def improve_readability(text, original_scores, gpt4_text, gpt4_scores, model, tokenizer, language):\n",
        "    if language == 'en':\n",
        "        prompt = f\"\"\"Improve the readability of the following text.\n",
        "\n",
        "Original text:\n",
        "{text}\n",
        "\n",
        "Original text metrics:\n",
        "    - BT_easiness: {original_scores['BT_easiness']:.2f}\n",
        "    - Flesch-Reading-Ease: {original_scores['Flesch-Reading-Ease']:.2f}\n",
        "    - SMOG Readability: {original_scores['SMOG Readability']:.2f}\n",
        "    - Automated Readability Index: {original_scores['Automated Readability Index']:.2f}\n",
        "\n",
        "GPT-4 improved version:\n",
        "{gpt4_text}\n",
        "\n",
        "GPT-4 improved version metrics:\n",
        "    - BT_easiness: {gpt4_scores['BT_easiness']:.2f}\n",
        "    - Flesch-Reading-Ease: {gpt4_scores['Flesch-Reading-Ease']:.2f}\n",
        "    - SMOG Readability: {gpt4_scores['SMOG Readability']:.2f}\n",
        "    - Automated Readability Index: {gpt4_scores['Automated Readability Index']:.2f}\n",
        "\n",
        "Now, provide your own improved version of the original text, aiming to match or exceed the GPT-4 version's readability scores:\n",
        "\n",
        "Improved text:\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"Verbessere die Lesbarkeit des folgenden Textes.\n",
        "\n",
        "Originaltext:\n",
        "{text}\n",
        "\n",
        "Metriken des Originaltextes:\n",
        "    - MOS Komplexität: {original_scores['mos_complexity']:.2f}\n",
        "    - Komplexitätsbewertungen: {original_scores['votes_complexity']}\n",
        "    - Verständlichkeitsbewertungen: {original_scores['votes_understandability']}\n",
        "    - Lexikalische Schwierigkeit: {original_scores['vote_lexical_difficulty']}\n",
        "\n",
        "Von GPT-4 verbesserte Version:\n",
        "{gpt4_text}\n",
        "\n",
        "Metriken der GPT-4 verbesserten Version:\n",
        "    - MOS Komplexität: {gpt4_scores['mos_complexity']:.2f}\n",
        "    - Komplexitätsbewertungen: {gpt4_scores['votes_complexity']}\n",
        "    - Verständlichkeitsbewertungen: {gpt4_scores['votes_understandability']}\n",
        "    - Lexikalische Schwierigkeit: {gpt4_scores['vote_lexical_difficulty']}\n",
        "\n",
        "Erstelle nun deine eigene verbesserte Version des Originaltextes und versuche, die Lesbarkeitsmetriken der GPT-4-Version zu erreichen oder zu übertreffen:\n",
        "\n",
        "Verbesserter Text:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.7, top_p=0.9)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load and combine datasets\n",
        "    combined_df = load_and_combine_datasets()\n",
        "\n",
        "    # Split the combined dataset into train and test sets\n",
        "    train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['language'], random_state=42)\n",
        "\n",
        "    # Prepare initial data\n",
        "    initial_data = prepare_data(train_df, sample_size=1000)\n",
        "\n",
        "    # Setup Llama 2 model\n",
        "    model_name = \"meta-llama/Llama-2-7b-hf\"  # Ensure you have access to this model\n",
        "    model, tokenizer = setup_llama2_model(model_name)\n",
        "\n",
        "    # Initial training\n",
        "    initial_training_data = [{\"text\": f\"\"\"{'Improve the readability of the following text' if item['language'] == 'en' else 'Verbessere die Lesbarkeit des folgenden Textes'}:\n",
        "\n",
        "{'Original text' if item['language'] == 'en' else 'Originaltext'}:\n",
        "{item['original_text']}\n",
        "\n",
        "{'Improved text' if item['language'] == 'en' else 'Verbesserter Text'}:\"\"\"} for item in initial_data]\n",
        "\n",
        "    model = train_llama2(model, tokenizer, initial_training_data, \"./llama2_initial_training_bilingual\")\n",
        "\n",
        "    # Feedback loop training\n",
        "    final_model = feedback_loop_training(model, tokenizer, initial_data, num_iterations=5)\n",
        "\n",
        "    # Save the final model\n",
        "    final_model.save_pretrained(\"./llama2_readability_improvement_final_with_feedback_bilingual\")\n",
        "    tokenizer.save_pretrained(\"./llama2_readability_improvement_final_with_feedback_bilingual\")\n",
        "\n",
        "    logging.info(\"Training completed. Final model saved.\")\n",
        "\n",
        "    # Example usage of the trained model\n",
        "    test_texts = [\n",
        "        {\"text\": \"The mitochondria is the powerhouse of the cell, responsible for producing energy through a process called cellular respiration.\", \"language\": \"en\"},\n",
        "        {\"text\": \"Die Mitochondrien sind die Kraftwerke der Zelle und produzieren Energie durch einen Prozess, der als zelluläre Atmung bezeichnet wird.\", \"language\": \"de\"}\n",
        "    ]\n",
        "\n",
        "    for test_item in test_texts:\n",
        "        original_scores = calculate_readability_scores(test_item['text'], test_item['language'])\n",
        "        gpt4_text = modify_text_gpt4(test_item['text'], original_scores, test_item['language'])\n",
        "        gpt4_scores = calculate_readability_scores(gpt4_text, test_item['language']) if test_item['language'] == 'en' else estimate_german_metrics(test_item['text'], gpt4_text, original_scores)\n",
        "\n",
        "        simplified_text = improve_readability(test_item['text'], original_scores, gpt4_text, gpt4_scores, final_model, tokenizer, test_item['language'])\n",
        "\n",
        "        print(f\"Original ({test_item['language']}): {test_item['text']}\")\n",
        "        print(f\"Simplified: {simplified_text}\")\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2Bjg6Gg4QUmo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}