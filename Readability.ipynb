{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHtOtT0BX6OUYMbQQ4y18W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar045/Assignment/blob/main/Readability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "import textstat\n",
        "import backoff\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Load and combine datasets\n",
        "def load_and_combine_datasets():\n",
        "    clear_df = pd.read_csv(\"path_to_your_CLEAR_dataset.csv\")\n",
        "    clear_df['dataset'] = 'CLEAR'\n",
        "    clear_df['language'] = 'en'\n",
        "\n",
        "    ratings_df = pd.read_csv(\"ratings.csv\", encoding=\"iso-8859-1\")\n",
        "    ratings_df.rename(lambda x: str(x).lower(), axis=\"columns\", inplace=True)\n",
        "    ratings_df['dataset'] = 'ratings'\n",
        "    ratings_df['language'] = 'de'\n",
        "    ratings_df.rename(columns={'sentence': 'Excerpt'}, inplace=True)\n",
        "\n",
        "    return pd.concat([clear_df, ratings_df], ignore_index=True)\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def estimate_bt_easiness(original_text):\n",
        "    prompt = f\"\"\"Estimate the BT_easiness score for the following text. BT_easiness is a measure of text readability,\n",
        "    where higher scores indicate easier-to-read text. The score typically ranges from 0 to 100.\n",
        "\n",
        "    Text:\n",
        "    {original_text}\n",
        "\n",
        "    Based on the complexity, vocabulary, and structure of the text, estimate the BT_easiness score.\n",
        "    Provide only the numeric score without any explanation.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    return float(response.choices[0].message.content.strip())\n",
        "\n",
        "def calculate_readability_scores(text, language, row=None):\n",
        "    if language == 'en':\n",
        "        bt_easiness = estimate_bt_easiness(text)\n",
        "        return {\n",
        "            \"BT_easiness\": bt_easiness,\n",
        "            \"Flesch-Reading-Ease\": textstat.flesch_reading_ease(text),\n",
        "            \"SMOG Readability\": textstat.smog_index(text),\n",
        "            \"Automated Readability Index\": textstat.automated_readability_index(text)\n",
        "        }\n",
        "    elif language == 'de':\n",
        "        return {\n",
        "            \"mos_complexity\": row['mos_complexity'] if row is not None else 0,\n",
        "            \"votes_complexity\": row['votes_complexity'] if row is not None else 0,\n",
        "            \"votes_understandability\": row['votes_understandability'] if row is not None else 0,\n",
        "            \"vote_lexical_difficulty\": row['vote_lexical_difficulty'] if row is not None else 0\n",
        "        }\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def modify_text_gpt4(text, current_scores, language):\n",
        "    if language == 'en':\n",
        "        prompt = f\"\"\"Improve the readability of the following text. Current metrics:\n",
        "        - BT_easiness: {current_scores['BT_easiness']:.2f}\n",
        "        - Flesch-Reading-Ease: {current_scores['Flesch-Reading-Ease']:.2f}\n",
        "        - SMOG Readability: {current_scores['SMOG Readability']:.2f}\n",
        "        - Automated Readability Index: {current_scores['Automated Readability Index']:.2f}\n",
        "\n",
        "    Original text:\n",
        "    {text}\n",
        "\n",
        "    Rewrite the text to improve all readability metrics. Aim for:\n",
        "        - Higher BT_easiness\n",
        "        - Higher Flesch-Reading-Ease\n",
        "        - Lower SMOG Readability\n",
        "        - Lower Automated Readability Index\n",
        "\n",
        "    Improved text:\"\"\"\n",
        "    elif language == 'de':\n",
        "        prompt = f\"\"\"Verbessere die Lesbarkeit des folgenden Textes. Aktuelle Metriken:\n",
        "        - MOS Komplexität: {current_scores['mos_complexity']:.2f}\n",
        "        - Komplexitätsbewertungen: {current_scores['votes_complexity']}\n",
        "        - Verständlichkeitsbewertungen: {current_scores['votes_understandability']}\n",
        "        - Lexikalische Schwierigkeit: {current_scores['vote_lexical_difficulty']}\n",
        "\n",
        "    Originaltext:\n",
        "    {text}\n",
        "\n",
        "    Schreibe den Text um, um alle Lesbarkeitsmetriken zu verbessern. Ziele:\n",
        "        - Niedrigere MOS Komplexität\n",
        "        - Weniger Komplexitätsbewertungen\n",
        "        - Mehr Verständlichkeitsbewertungen\n",
        "        - Niedrigere lexikalische Schwierigkeit\n",
        "\n",
        "    Verbesserter Text:\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def estimate_german_metrics(original_text, modified_text, original_scores):\n",
        "    prompt = f\"\"\"Given the original German text and its improved version, estimate the new readability metrics.\n",
        "    The original metrics were:\n",
        "    - MOS Komplexität: {original_scores['mos_complexity']:.2f}\n",
        "    - Komplexitätsbewertungen: {original_scores['votes_complexity']}\n",
        "    - Verständlichkeitsbewertungen: {original_scores['votes_understandability']}\n",
        "    - Lexikalische Schwierigkeit: {original_scores['vote_lexical_difficulty']}\n",
        "\n",
        "    Original text:\n",
        "    {original_text}\n",
        "\n",
        "    Improved text:\n",
        "    {modified_text}\n",
        "\n",
        "    Estimate the new metrics. They should show improvement over the original scores.\n",
        "    Provide only the numeric scores in the following format:\n",
        "    MOS Komplexität: [value]\n",
        "    Komplexitätsbewertungen: [value]\n",
        "    Verständlichkeitsbewertungen: [value]\n",
        "    Lexikalische Schwierigkeit: [value]\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "    )\n",
        "\n",
        "    lines = response.choices[0].message.content.strip().split('\\n')\n",
        "    new_scores = {}\n",
        "    for line in lines:\n",
        "        key, value = line.split(':')\n",
        "        new_scores[key.strip()] = float(value.strip())\n",
        "\n",
        "    return {\n",
        "        \"mos_complexity\": new_scores[\"MOS Komplexität\"],\n",
        "        \"votes_complexity\": int(new_scores[\"Komplexitätsbewertungen\"]),\n",
        "        \"votes_understandability\": int(new_scores[\"Verständlichkeitsbewertungen\"]),\n",
        "        \"vote_lexical_difficulty\": int(new_scores[\"Lexikalische Schwierigkeit\"])\n",
        "    }\n",
        "\n",
        "def prepare_data(df, sample_size=1000):\n",
        "    df_subset = df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "    prepared_data = []\n",
        "\n",
        "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=\"Preparing data\"):\n",
        "        original_scores = calculate_readability_scores(row['Excerpt'], row['language'], row)\n",
        "\n",
        "        gpt4_modified_text = modify_text_gpt4(row['Excerpt'], original_scores, row['language'])\n",
        "        if row['language'] == 'en':\n",
        "            gpt4_scores = calculate_readability_scores(gpt4_modified_text, row['language'])\n",
        "        else:\n",
        "            gpt4_scores = estimate_german_metrics(row['Excerpt'], gpt4_modified_text, original_scores)\n",
        "\n",
        "        prepared_data.append({\n",
        "            \"instruction\": \"Simplify the following text while preserving its meaning:\",\n",
        "            \"input\": row['Excerpt'],\n",
        "            \"output\": gpt4_modified_text,\n",
        "            \"original_scores\": original_scores,\n",
        "            \"gpt4_scores\": gpt4_scores,\n",
        "            \"language\": row['language']\n",
        "        })\n",
        "\n",
        "    return prepared_data\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction=instruction, input=input, output=output)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "def evaluate_improvement(original_text, gpt4_text, llama3_text, original_scores, gpt4_scores, language):\n",
        "    if language == 'en':\n",
        "        llama3_scores = calculate_readability_scores(llama3_text, language)\n",
        "    else:\n",
        "        llama3_scores = estimate_german_metrics(original_text, llama3_text, original_scores)\n",
        "\n",
        "    improvements = {}\n",
        "    metrics = list(original_scores.keys())\n",
        "    for metric in metrics:\n",
        "        original_val = original_scores[metric]\n",
        "        gpt4_val = gpt4_scores[metric]\n",
        "        llama3_val = llama3_scores[metric]\n",
        "\n",
        "        if language == 'en' and metric in ['BT_easiness', 'Flesch-Reading-Ease']:\n",
        "            gpt4_improvement = (gpt4_val - original_val) / original_val * 100\n",
        "            llama3_improvement = (llama3_val - original_val) / original_val * 100\n",
        "        elif language == 'de' and metric in ['votes_understandability']:\n",
        "            gpt4_improvement = (gpt4_val - original_val) / original_val * 100\n",
        "            llama3_improvement = (llama3_val - original_val) / original_val * 100\n",
        "        else:\n",
        "            gpt4_improvement = (original_val - gpt4_val) / original_val * 100\n",
        "            llama3_improvement = (original_val - llama3_val) / original_val * 100\n",
        "\n",
        "        improvements[metric] = {\n",
        "            'gpt4': gpt4_improvement,\n",
        "            'llama3': llama3_improvement\n",
        "        }\n",
        "\n",
        "    return llama3_scores, improvements\n",
        "\n",
        "def generate_feedback(improvements, language):\n",
        "    feedback = \"Based on the improvements:\\n\" if language == 'en' else \"Basierend auf den Verbesserungen:\\n\"\n",
        "    for metric, values in improvements.items():\n",
        "        if values['llama3'] >= values['gpt4']:\n",
        "            feedback += f\"- {metric}: {'Great job! You\\'ve matched or exceeded GPT-4\\'s improvement.' if language == 'en' else 'Großartig! Sie haben die Verbesserung von GPT-4 erreicht oder übertroffen.'}\\n\"\n",
        "        else:\n",
        "            feedback += f\"- {metric}: {'There\\'s room for improvement. Try to match GPT-4\\'s performance.' if language == 'en' else 'Es gibt Raum für Verbesserungen. Versuchen Sie, die Leistung von GPT-4 zu erreichen.'}\\n\"\n",
        "    return feedback\n",
        "\n",
        "@backoff.on_exception(backoff.expo, OpenAI.error.RateLimitError)\n",
        "def get_gpt4_analysis(original_text, gpt4_text, llama3_text, improvements, language):\n",
        "    prompt = f\"\"\"Analyze the readability improvements made by LLaMA 3 compared to GPT-4 for the following {'English' if language == 'en' else 'German'} text.\n",
        "\n",
        "Original text:\n",
        "{original_text}\n",
        "\n",
        "GPT-4 improved version:\n",
        "{gpt4_text}\n",
        "\n",
        "LLaMA 3 improved version:\n",
        "{llama3_text}\n",
        "\n",
        "Improvement percentages:\n",
        "{improvements}\n",
        "\n",
        "Provide a detailed analysis of LLaMA 3's performance:\n",
        "1. What did LLaMA 3 do well in improving readability?\n",
        "2. Where did LLaMA 3 fall short compared to GPT-4?\n",
        "3. Suggest specific strategies for LLaMA 3 to improve its performance.\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def feedback_loop_training(model, tokenizer, dataset, num_iterations=5):\n",
        "    for iteration in range(num_iterations):\n",
        "        logging.info(f\"Starting iteration {iteration + 1}\")\n",
        "\n",
        "        new_training_data = []\n",
        "        for item in tqdm(dataset, desc=f\"Processing iteration {iteration + 1}\"):\n",
        "            llama3_text = model.generate(\n",
        "                **tokenizer(item['input'], return_tensors=\"pt\", max_length=1024, truncation=True),\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "            llama3_text = tokenizer.decode(llama3_text[0], skip_special_tokens=True)\n",
        "\n",
        "            llama3_scores, improvements = evaluate_improvement(\n",
        "                item['input'], item['output'], llama3_text,\n",
        "                item['original_scores'], item['gpt4_scores'], item['language']\n",
        "            )\n",
        "\n",
        "            feedback = generate_feedback(improvements, item['language'])\n",
        "            gpt4_analysis = get_gpt4_analysis(item['input'], item['output'], llama3_text, improvements, item['language'])\n",
        "\n",
        "            new_example = f\"\"\"{'Original text' if item['language'] == 'en' else 'Originaltext'}: {item['input']}\n",
        "\n",
        "{'GPT-4 improved version' if item['language'] == 'en' else 'Von GPT-4 verbesserte Version'}: {item['output']}\n",
        "\n",
        "{'Your previous improvement' if item['language'] == 'en' else 'Ihre vorherige Verbesserung'}: {llama3_text}\n",
        "\n",
        "{'Feedback' if item['language'] == 'en' else 'Rückmeldung'}: {feedback}\n",
        "\n",
        "{'Expert analysis' if item['language'] == 'en' else 'Expertenanalyse'}: {gpt4_analysis}\n",
        "\n",
        "{'Now, provide an improved version addressing the feedback and analysis' if item['language'] == 'en' else 'Geben Sie nun eine verbesserte Version an, die auf das Feedback und die Analyse eingeht'}:\n",
        "\n",
        "{'Improved text' if item['language'] == 'en' else 'Verbesserter Text'}:\"\"\"\n",
        "\n",
        "            new_training_data.append({\"text\": new_example})\n",
        "\n",
        "        new_dataset = Dataset.from_pandas(pd.DataFrame(new_training_data))\n",
        "\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=new_dataset,\n",
        "            dataset_text_field=\"text\",\n",
        "            max_seq_length=1024,\n",
        "            dataset_num_proc=2,\n",
        "            packing=False,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=2,\n",
        "                gradient_accumulation_steps=4,\n",
        "                warmup_steps=5,\n",
        "                max_steps=60,\n",
        "                learning_rate=2e-4,\n",
        "                fp16=not FastLanguageModel.is_bfloat16_supported(),\n",
        "                bf16=FastLanguageModel.is_bfloat16_supported(),\n",
        "                logging_steps=1,\n",
        "                optim=\"adamw_8bit\",\n",
        "                weight_decay=0.01,\n",
        "                lr_scheduler_type=\"linear\",\n",
        "                seed=3407,\n",
        "                output_dir=f\"outputs_iteration_{iteration+1}\",\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        model = trainer.model\n",
        "\n",
        "        logging.info(f\"Completed iteration {iteration + 1}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # Load and combine datasets\n",
        "    combined_df = load_and_combine_datasets()\n",
        "\n",
        "    # Split the combined dataset into train and test sets\n",
        "    train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['language'], random_state=42)\n",
        "\n",
        "    # Prepare initial data\n",
        "    prepared_data = prepare_data(train_df, sample_size=1000)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_pandas(pd.DataFrame(prepared_data))\n",
        "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "    # Setup LLaMA 3.1 model\n",
        "    max_seq_length = 1024\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Setup PEFT (Parameter-Efficient Fine-Tuning)\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "    )\n",
        "\n",
        "    # Initial training\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=5,\n",
        "            max_steps=60,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=not FastLanguageModel.is_bfloat16_supported(),\n",
        "            bf16=FastLanguageModel.is_bfloat16_supported(),\n",
        "            logging_steps=1,\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            seed=3407,\n",
        "            output_dir=\"outputs_initial\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Feedback loop training\n",
        "    final_model = feedback_loop_training(trainer.model, tokenizer, dataset, num_iterations=5)\n",
        "\n",
        "    # Save the final model\n",
        "    final_model.save_pretrained(\"llama3_readability_improvement_final_with_feedback\")\n",
        "    tokenizer.save_pretrained(\"llama3_readability_improvement_final_with_feedback\")\n",
        "\n",
        "    logging.info(\"Training completed. Final model saved.\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_data = prepare_data(test_df, sample_size=100)  # Using a smaller sample for quick testing\n",
        "\n",
        "    for item in test_data:\n",
        "        llama3_text = final_model.generate(\n",
        "            **tokenizer(item['input'], return_tensors=\"pt\", max_length=1024, truncation=True),\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9\n",
        "        )\n",
        "        llama3_text = tokenizer.decode(llama3_text[0], skip_special_tokens=True)\n",
        "\n",
        "        llama3_scores, improvements = evaluate_improvement(\n",
        "            item['input'], item['output'], llama3_text,\n",
        "            item['original_scores'], item['gpt4_scores'], item['language']\n",
        "        )\n",
        "\n",
        "        print(f\"Original ({item['language']}): {item['input']}\")\n",
        "        print(f\"GPT-4 Simplified: {item['output']}\")\n",
        "        print(f\"LLaMA 3 Simplified: {llama3_text}\")\n",
        "        print(\"Improvements:\", improvements)\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "BM2-fuF5-i8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}